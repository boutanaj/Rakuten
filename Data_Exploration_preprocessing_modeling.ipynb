{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25ae6451-39c9-40ab-ad19-9cab9c34d73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32df7bac-0fc0-427f-8ef1-cb1396dbfbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Xtrain = pd.read_csv(\"Data/x_train_update.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eb64e81-6bdb-4c71-85e7-ab0c5ab0320e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3804725264</td>\n",
       "      <td>1263597046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>436067568</td>\n",
       "      <td>1008141237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n",
       "      <td>PILOT STYLE Touch Pen de marque Speedlink est ...</td>\n",
       "      <td>201115110</td>\n",
       "      <td>938777978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Peluche Donald - Europe - Disneyland 2000 (Mar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50418756</td>\n",
       "      <td>457047496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>La Guerre Des Tuques</td>\n",
       "      <td>Luc a des id&amp;eacute;es de grandeur. Il veut or...</td>\n",
       "      <td>278535884</td>\n",
       "      <td>1077757786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        designation  \\\n",
       "0           0  Olivia: Personalisiertes Notizbuch / 150 Seite...   \n",
       "1           1  Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...   \n",
       "2           2  Grand Stylet Ergonomique Bleu Gamepad Nintendo...   \n",
       "3           3  Peluche Donald - Europe - Disneyland 2000 (Mar...   \n",
       "4           4                               La Guerre Des Tuques   \n",
       "\n",
       "                                         description   productid     imageid  \n",
       "0                                                NaN  3804725264  1263597046  \n",
       "1                                                NaN   436067568  1008141237  \n",
       "2  PILOT STYLE Touch Pen de marque Speedlink est ...   201115110   938777978  \n",
       "3                                                NaN    50418756   457047496  \n",
       "4  Luc a des id&eacute;es de grandeur. Il veut or...   278535884  1077757786  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Xtrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d05a8c50-7abb-437a-93a3-89ec963075b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84916, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a4c5ab8-b35a-4c14-9d00-53bde78339db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0         0\n",
       "designation        0\n",
       "description    29800\n",
       "productid          0\n",
       "imageid            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Xtrain.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56942308-e8ae-4c1d-8279-75a81bcc83ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727e8cc5-ae1c-4e05-b4fb-14b6a30556ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4f1963-22ad-4e96-b12e-8eb22b4701fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c35e7e4-0bde-4688-93b3-9a12bb25278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Xtest = pd.read_csv(\"Data/x_test_update.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe120fca-1552-4590-830d-51bc290c570c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84916</td>\n",
       "      <td>Folkmanis Puppets - 2732 - Marionnette Et Théâ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>516376098</td>\n",
       "      <td>1019294171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84917</td>\n",
       "      <td>Porte Flamme Gaxix - Flamebringer Gaxix - 136/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>133389013</td>\n",
       "      <td>1274228667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84918</td>\n",
       "      <td>Pompe de filtration Speck Badu 95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4128438366</td>\n",
       "      <td>1295960357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84919</td>\n",
       "      <td>Robot de piscine électrique</td>\n",
       "      <td>&lt;p&gt;Ce robot de piscine d&amp;#39;un design innovan...</td>\n",
       "      <td>3929899732</td>\n",
       "      <td>1265224052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84920</td>\n",
       "      <td>Hsm Destructeur Securio C16 Coupe Crois¿E: 4 X...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>152993898</td>\n",
       "      <td>940543690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        designation  \\\n",
       "0       84916  Folkmanis Puppets - 2732 - Marionnette Et Théâ...   \n",
       "1       84917  Porte Flamme Gaxix - Flamebringer Gaxix - 136/...   \n",
       "2       84918                  Pompe de filtration Speck Badu 95   \n",
       "3       84919                        Robot de piscine électrique   \n",
       "4       84920  Hsm Destructeur Securio C16 Coupe Crois¿E: 4 X...   \n",
       "\n",
       "                                         description   productid     imageid  \n",
       "0                                                NaN   516376098  1019294171  \n",
       "1                                                NaN   133389013  1274228667  \n",
       "2                                                NaN  4128438366  1295960357  \n",
       "3  <p>Ce robot de piscine d&#39;un design innovan...  3929899732  1265224052  \n",
       "4                                                NaN   152993898   940543690  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Xtest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9af7404-23d5-485d-a06f-3d0a53d42b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13812, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95888f8d-3110-4599-a271-30367c8bbe3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0        0\n",
       "designation       0\n",
       "description    4886\n",
       "productid         0\n",
       "imageid           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Xtest.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa317c7a-eddc-4cf2-884f-572ba321c616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "068563b7-6ed6-4f9a-94b7-fff512b770f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Ytrain = pd.read_csv(\"Data/Y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83067b10-8d2e-4027-92e1-d4e73c07e0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Ytest = pd.read_csv(\"Data/Y_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46213650-e621-4137-974a-a3e6a622f0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13812, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1515a4a5-fff0-4ba9-817f-c794b93425bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>prdtypecode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84916</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84917</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84918</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84919</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84920</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  prdtypecode\n",
       "0       84916           10\n",
       "1       84917           10\n",
       "2       84918           10\n",
       "3       84919           10\n",
       "4       84920           10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Ytrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5907bb44-5970-4d6d-8622-9c14d026315f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0     0\n",
       "prdtypecode    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Ytrain.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eaf241-b9f7-4c67-878e-7885021c598d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd91d3be-3542-4e7d-9b21-5e83210e6237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84916, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1673a09-561f-40d4-83a5-3186286c85a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>prdtypecode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  prdtypecode\n",
       "0           0           10\n",
       "1           1         2280\n",
       "2           2           50\n",
       "3           3         1280\n",
       "4           4         2705"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Ytest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52495f29-3f1b-478e-9567-c6fa7c5d18a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0     0\n",
       "prdtypecode    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Ytest.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ac87da-2dd4-448e-8a18-cfd717ea676c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755d03fa-b64e-48db-9059-b48dce77544c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20b732d1-cd4e-4837-994c-630ef3244c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_Xtrain_image_sample1 = pd.read_csv(\"Data/images/image_train/image_528113_product_923222.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb3a6b-d0f2-4b35-9983-c78f7a8e6695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8cce237-deeb-4a39-854b-5368e0df4e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'designation', 'description', 'productid', 'imageid'], dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Xtrain.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31484a20-aca7-446c-8d58-ed2693b3b63e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'designation', 'description', 'productid', 'imageid'], dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Xtest.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8569776d-687a-466e-ac13-1c5d164252a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'prdtypecode'], dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Ytrain.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e20d6840-5429-4712-850b-47fcead1545e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'prdtypecode'], dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Ytest.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536fbf9a-b37f-4462-a0bf-67ea33b1b815",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac2d85a-ab95-4486-8e7a-c35eccf9ae2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des données...\n",
      "Données chargées avec succès !\n",
      "Traitement des valeurs manquantes...\n",
      "Valeurs manquantes traitées !\n",
      "Fusion des datasets...\n",
      "Fusion terminée !\n",
      "Création du pipeline et début de l'entraînement...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ✅ Chargement des données\n",
    "print(\"Chargement des données...\")\n",
    "df_Xtrain = pd.read_csv(\"Data/X_train_update.csv\")\n",
    "df_Xtest = pd.read_csv(\"Data/X_test_update.csv\")\n",
    "df_Ytrain = pd.read_csv(\"Data/Y_train.csv\")\n",
    "df_Ytest = pd.read_csv(\"Data/Y_test.csv\")\n",
    "print(\"Données chargées avec succès !\")\n",
    "\n",
    "# ✅ Correction du warning en évitant inplace=True\n",
    "print(\"Traitement des valeurs manquantes...\")\n",
    "df_Xtrain = df_Xtrain.copy()\n",
    "df_Xtest = df_Xtest.copy()\n",
    "\n",
    "df_Xtrain['description'] = df_Xtrain['description'].fillna(\"aucune description\")\n",
    "df_Xtest['description'] = df_Xtest['description'].fillna(\"aucune description\")\n",
    "print(\"Valeurs manquantes traitées !\")\n",
    "\n",
    "# ✅ Fusion des features avec les labels\n",
    "print(\"Fusion des datasets...\")\n",
    "df_train = df_Xtrain.merge(df_Ytrain, on=\"Unnamed: 0\").drop(columns=[\"Unnamed: 0\"])\n",
    "df_test = df_Xtest.merge(df_Ytest, on=\"Unnamed: 0\").drop(columns=[\"Unnamed: 0\"])\n",
    "print(\"Fusion terminée !\")\n",
    "\n",
    "# ✅ Création des variables X (features) et y (labels)\n",
    "X_train_text = df_train[\"description\"]\n",
    "X_test_text = df_test[\"description\"]\n",
    "y_train = df_train[\"prdtypecode\"]\n",
    "y_test = df_test[\"prdtypecode\"]\n",
    "\n",
    "# ✅ Création d'un pipeline avec un vectoriseur TF-IDF et un modèle de classification\n",
    "print(\"Création du pipeline et début de l'entraînement...\")\n",
    "pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(stop_words=None, max_features=10000)),  # Vectorisation des descriptions\n",
    "    (\"classifier\", RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1))  # Modèle\n",
    "])\n",
    "\n",
    "#from nltk.corpus import stopwords\n",
    "#stop_words = stopwords.words('french')\n",
    "#vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "# ✅ Mesure du temps d'entraînement\n",
    "start_time = time.time()\n",
    "pipeline.fit(X_train_text, y_train)\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Modèle entraîné en {training_time:.2f} secondes.\")\n",
    "\n",
    "# ✅ Prédictions et évaluation\n",
    "print(\"Prédiction et évaluation du modèle...\")\n",
    "y_pred = pipeline.predict(X_test_text)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Précision du modèle : {accuracy:.4f}\")\n",
    "\n",
    "print(\"Fin du processus ! 🎯\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ba2554-01b0-4d19-a604-de578c9efc5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e41911c-6d1b-454e-b205-bb0ca543f1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision transformers Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c94e5c3-33c3-45a3-bd7d-f4b4bd9a6cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92adb20e-4c3d-4a31-8db4-5a7ba4bd2d37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5e608d-a25d-480e-923f-ff55d1c0f437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651d8d08-b25c-4307-9783-faae8946989a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64d6ee9-67c5-408e-9315-16703266a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traintement des données textuels et visuels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5820b880-56af-41bc-8078-a6d628847766",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade optree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2be7b0-216a-404e-a85e-381c0de3fe3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285d8faf-515f-40af-ac83-ddc8878a4596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4f02e4-3e69-4708-b866-d7bc923d88fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dd94d8-c008-4c57-8692-15e6bd01b4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68dc3eee-e5a4-445c-b31d-2ab0f6413d70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des données...\n",
      "Données chargées avec succès !\n",
      "Traitement des valeurs manquantes...\n",
      "Valeurs manquantes traitées !\n",
      "Fusion des datasets...\n",
      "Fusion terminée !\n",
      "Chargement du modèle BLIP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a740407\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "`BlipModel` is going to be deprecated in future release, please use `BlipForConditionalGeneration`, `BlipForQuestionAnswering` or `BlipForImageTextRetrieval` depending on your usecase.\n",
      "Some weights of BlipModel were not initialized from the model checkpoint at Salesforce/blip-image-captioning-base and are newly initialized: ['logit_scale', 'text_model.embeddings.LayerNorm.bias', 'text_model.embeddings.LayerNorm.weight', 'text_model.embeddings.position_embeddings.weight', 'text_model.embeddings.word_embeddings.weight', 'text_model.encoder.layer.0.attention.output.LayerNorm.bias', 'text_model.encoder.layer.0.attention.output.LayerNorm.weight', 'text_model.encoder.layer.0.attention.output.dense.bias', 'text_model.encoder.layer.0.attention.output.dense.weight', 'text_model.encoder.layer.0.attention.self.key.bias', 'text_model.encoder.layer.0.attention.self.key.weight', 'text_model.encoder.layer.0.attention.self.query.bias', 'text_model.encoder.layer.0.attention.self.query.weight', 'text_model.encoder.layer.0.attention.self.value.bias', 'text_model.encoder.layer.0.attention.self.value.weight', 'text_model.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.0.crossattention.output.dense.bias', 'text_model.encoder.layer.0.crossattention.output.dense.weight', 'text_model.encoder.layer.0.crossattention.self.key.bias', 'text_model.encoder.layer.0.crossattention.self.key.weight', 'text_model.encoder.layer.0.crossattention.self.query.bias', 'text_model.encoder.layer.0.crossattention.self.query.weight', 'text_model.encoder.layer.0.crossattention.self.value.bias', 'text_model.encoder.layer.0.crossattention.self.value.weight', 'text_model.encoder.layer.0.intermediate.dense.bias', 'text_model.encoder.layer.0.intermediate.dense.weight', 'text_model.encoder.layer.0.output.LayerNorm.bias', 'text_model.encoder.layer.0.output.LayerNorm.weight', 'text_model.encoder.layer.0.output.dense.bias', 'text_model.encoder.layer.0.output.dense.weight', 'text_model.encoder.layer.1.attention.output.LayerNorm.bias', 'text_model.encoder.layer.1.attention.output.LayerNorm.weight', 'text_model.encoder.layer.1.attention.output.dense.bias', 'text_model.encoder.layer.1.attention.output.dense.weight', 'text_model.encoder.layer.1.attention.self.key.bias', 'text_model.encoder.layer.1.attention.self.key.weight', 'text_model.encoder.layer.1.attention.self.query.bias', 'text_model.encoder.layer.1.attention.self.query.weight', 'text_model.encoder.layer.1.attention.self.value.bias', 'text_model.encoder.layer.1.attention.self.value.weight', 'text_model.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.1.crossattention.output.dense.bias', 'text_model.encoder.layer.1.crossattention.output.dense.weight', 'text_model.encoder.layer.1.crossattention.self.key.bias', 'text_model.encoder.layer.1.crossattention.self.key.weight', 'text_model.encoder.layer.1.crossattention.self.query.bias', 'text_model.encoder.layer.1.crossattention.self.query.weight', 'text_model.encoder.layer.1.crossattention.self.value.bias', 'text_model.encoder.layer.1.crossattention.self.value.weight', 'text_model.encoder.layer.1.intermediate.dense.bias', 'text_model.encoder.layer.1.intermediate.dense.weight', 'text_model.encoder.layer.1.output.LayerNorm.bias', 'text_model.encoder.layer.1.output.LayerNorm.weight', 'text_model.encoder.layer.1.output.dense.bias', 'text_model.encoder.layer.1.output.dense.weight', 'text_model.encoder.layer.10.attention.output.LayerNorm.bias', 'text_model.encoder.layer.10.attention.output.LayerNorm.weight', 'text_model.encoder.layer.10.attention.output.dense.bias', 'text_model.encoder.layer.10.attention.output.dense.weight', 'text_model.encoder.layer.10.attention.self.key.bias', 'text_model.encoder.layer.10.attention.self.key.weight', 'text_model.encoder.layer.10.attention.self.query.bias', 'text_model.encoder.layer.10.attention.self.query.weight', 'text_model.encoder.layer.10.attention.self.value.bias', 'text_model.encoder.layer.10.attention.self.value.weight', 'text_model.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.10.crossattention.output.dense.bias', 'text_model.encoder.layer.10.crossattention.output.dense.weight', 'text_model.encoder.layer.10.crossattention.self.key.bias', 'text_model.encoder.layer.10.crossattention.self.key.weight', 'text_model.encoder.layer.10.crossattention.self.query.bias', 'text_model.encoder.layer.10.crossattention.self.query.weight', 'text_model.encoder.layer.10.crossattention.self.value.bias', 'text_model.encoder.layer.10.crossattention.self.value.weight', 'text_model.encoder.layer.10.intermediate.dense.bias', 'text_model.encoder.layer.10.intermediate.dense.weight', 'text_model.encoder.layer.10.output.LayerNorm.bias', 'text_model.encoder.layer.10.output.LayerNorm.weight', 'text_model.encoder.layer.10.output.dense.bias', 'text_model.encoder.layer.10.output.dense.weight', 'text_model.encoder.layer.11.attention.output.LayerNorm.bias', 'text_model.encoder.layer.11.attention.output.LayerNorm.weight', 'text_model.encoder.layer.11.attention.output.dense.bias', 'text_model.encoder.layer.11.attention.output.dense.weight', 'text_model.encoder.layer.11.attention.self.key.bias', 'text_model.encoder.layer.11.attention.self.key.weight', 'text_model.encoder.layer.11.attention.self.query.bias', 'text_model.encoder.layer.11.attention.self.query.weight', 'text_model.encoder.layer.11.attention.self.value.bias', 'text_model.encoder.layer.11.attention.self.value.weight', 'text_model.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.11.crossattention.output.dense.bias', 'text_model.encoder.layer.11.crossattention.output.dense.weight', 'text_model.encoder.layer.11.crossattention.self.key.bias', 'text_model.encoder.layer.11.crossattention.self.key.weight', 'text_model.encoder.layer.11.crossattention.self.query.bias', 'text_model.encoder.layer.11.crossattention.self.query.weight', 'text_model.encoder.layer.11.crossattention.self.value.bias', 'text_model.encoder.layer.11.crossattention.self.value.weight', 'text_model.encoder.layer.11.intermediate.dense.bias', 'text_model.encoder.layer.11.intermediate.dense.weight', 'text_model.encoder.layer.11.output.LayerNorm.bias', 'text_model.encoder.layer.11.output.LayerNorm.weight', 'text_model.encoder.layer.11.output.dense.bias', 'text_model.encoder.layer.11.output.dense.weight', 'text_model.encoder.layer.2.attention.output.LayerNorm.bias', 'text_model.encoder.layer.2.attention.output.LayerNorm.weight', 'text_model.encoder.layer.2.attention.output.dense.bias', 'text_model.encoder.layer.2.attention.output.dense.weight', 'text_model.encoder.layer.2.attention.self.key.bias', 'text_model.encoder.layer.2.attention.self.key.weight', 'text_model.encoder.layer.2.attention.self.query.bias', 'text_model.encoder.layer.2.attention.self.query.weight', 'text_model.encoder.layer.2.attention.self.value.bias', 'text_model.encoder.layer.2.attention.self.value.weight', 'text_model.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.2.crossattention.output.dense.bias', 'text_model.encoder.layer.2.crossattention.output.dense.weight', 'text_model.encoder.layer.2.crossattention.self.key.bias', 'text_model.encoder.layer.2.crossattention.self.key.weight', 'text_model.encoder.layer.2.crossattention.self.query.bias', 'text_model.encoder.layer.2.crossattention.self.query.weight', 'text_model.encoder.layer.2.crossattention.self.value.bias', 'text_model.encoder.layer.2.crossattention.self.value.weight', 'text_model.encoder.layer.2.intermediate.dense.bias', 'text_model.encoder.layer.2.intermediate.dense.weight', 'text_model.encoder.layer.2.output.LayerNorm.bias', 'text_model.encoder.layer.2.output.LayerNorm.weight', 'text_model.encoder.layer.2.output.dense.bias', 'text_model.encoder.layer.2.output.dense.weight', 'text_model.encoder.layer.3.attention.output.LayerNorm.bias', 'text_model.encoder.layer.3.attention.output.LayerNorm.weight', 'text_model.encoder.layer.3.attention.output.dense.bias', 'text_model.encoder.layer.3.attention.output.dense.weight', 'text_model.encoder.layer.3.attention.self.key.bias', 'text_model.encoder.layer.3.attention.self.key.weight', 'text_model.encoder.layer.3.attention.self.query.bias', 'text_model.encoder.layer.3.attention.self.query.weight', 'text_model.encoder.layer.3.attention.self.value.bias', 'text_model.encoder.layer.3.attention.self.value.weight', 'text_model.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.3.crossattention.output.dense.bias', 'text_model.encoder.layer.3.crossattention.output.dense.weight', 'text_model.encoder.layer.3.crossattention.self.key.bias', 'text_model.encoder.layer.3.crossattention.self.key.weight', 'text_model.encoder.layer.3.crossattention.self.query.bias', 'text_model.encoder.layer.3.crossattention.self.query.weight', 'text_model.encoder.layer.3.crossattention.self.value.bias', 'text_model.encoder.layer.3.crossattention.self.value.weight', 'text_model.encoder.layer.3.intermediate.dense.bias', 'text_model.encoder.layer.3.intermediate.dense.weight', 'text_model.encoder.layer.3.output.LayerNorm.bias', 'text_model.encoder.layer.3.output.LayerNorm.weight', 'text_model.encoder.layer.3.output.dense.bias', 'text_model.encoder.layer.3.output.dense.weight', 'text_model.encoder.layer.4.attention.output.LayerNorm.bias', 'text_model.encoder.layer.4.attention.output.LayerNorm.weight', 'text_model.encoder.layer.4.attention.output.dense.bias', 'text_model.encoder.layer.4.attention.output.dense.weight', 'text_model.encoder.layer.4.attention.self.key.bias', 'text_model.encoder.layer.4.attention.self.key.weight', 'text_model.encoder.layer.4.attention.self.query.bias', 'text_model.encoder.layer.4.attention.self.query.weight', 'text_model.encoder.layer.4.attention.self.value.bias', 'text_model.encoder.layer.4.attention.self.value.weight', 'text_model.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.4.crossattention.output.dense.bias', 'text_model.encoder.layer.4.crossattention.output.dense.weight', 'text_model.encoder.layer.4.crossattention.self.key.bias', 'text_model.encoder.layer.4.crossattention.self.key.weight', 'text_model.encoder.layer.4.crossattention.self.query.bias', 'text_model.encoder.layer.4.crossattention.self.query.weight', 'text_model.encoder.layer.4.crossattention.self.value.bias', 'text_model.encoder.layer.4.crossattention.self.value.weight', 'text_model.encoder.layer.4.intermediate.dense.bias', 'text_model.encoder.layer.4.intermediate.dense.weight', 'text_model.encoder.layer.4.output.LayerNorm.bias', 'text_model.encoder.layer.4.output.LayerNorm.weight', 'text_model.encoder.layer.4.output.dense.bias', 'text_model.encoder.layer.4.output.dense.weight', 'text_model.encoder.layer.5.attention.output.LayerNorm.bias', 'text_model.encoder.layer.5.attention.output.LayerNorm.weight', 'text_model.encoder.layer.5.attention.output.dense.bias', 'text_model.encoder.layer.5.attention.output.dense.weight', 'text_model.encoder.layer.5.attention.self.key.bias', 'text_model.encoder.layer.5.attention.self.key.weight', 'text_model.encoder.layer.5.attention.self.query.bias', 'text_model.encoder.layer.5.attention.self.query.weight', 'text_model.encoder.layer.5.attention.self.value.bias', 'text_model.encoder.layer.5.attention.self.value.weight', 'text_model.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.5.crossattention.output.dense.bias', 'text_model.encoder.layer.5.crossattention.output.dense.weight', 'text_model.encoder.layer.5.crossattention.self.key.bias', 'text_model.encoder.layer.5.crossattention.self.key.weight', 'text_model.encoder.layer.5.crossattention.self.query.bias', 'text_model.encoder.layer.5.crossattention.self.query.weight', 'text_model.encoder.layer.5.crossattention.self.value.bias', 'text_model.encoder.layer.5.crossattention.self.value.weight', 'text_model.encoder.layer.5.intermediate.dense.bias', 'text_model.encoder.layer.5.intermediate.dense.weight', 'text_model.encoder.layer.5.output.LayerNorm.bias', 'text_model.encoder.layer.5.output.LayerNorm.weight', 'text_model.encoder.layer.5.output.dense.bias', 'text_model.encoder.layer.5.output.dense.weight', 'text_model.encoder.layer.6.attention.output.LayerNorm.bias', 'text_model.encoder.layer.6.attention.output.LayerNorm.weight', 'text_model.encoder.layer.6.attention.output.dense.bias', 'text_model.encoder.layer.6.attention.output.dense.weight', 'text_model.encoder.layer.6.attention.self.key.bias', 'text_model.encoder.layer.6.attention.self.key.weight', 'text_model.encoder.layer.6.attention.self.query.bias', 'text_model.encoder.layer.6.attention.self.query.weight', 'text_model.encoder.layer.6.attention.self.value.bias', 'text_model.encoder.layer.6.attention.self.value.weight', 'text_model.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.6.crossattention.output.dense.bias', 'text_model.encoder.layer.6.crossattention.output.dense.weight', 'text_model.encoder.layer.6.crossattention.self.key.bias', 'text_model.encoder.layer.6.crossattention.self.key.weight', 'text_model.encoder.layer.6.crossattention.self.query.bias', 'text_model.encoder.layer.6.crossattention.self.query.weight', 'text_model.encoder.layer.6.crossattention.self.value.bias', 'text_model.encoder.layer.6.crossattention.self.value.weight', 'text_model.encoder.layer.6.intermediate.dense.bias', 'text_model.encoder.layer.6.intermediate.dense.weight', 'text_model.encoder.layer.6.output.LayerNorm.bias', 'text_model.encoder.layer.6.output.LayerNorm.weight', 'text_model.encoder.layer.6.output.dense.bias', 'text_model.encoder.layer.6.output.dense.weight', 'text_model.encoder.layer.7.attention.output.LayerNorm.bias', 'text_model.encoder.layer.7.attention.output.LayerNorm.weight', 'text_model.encoder.layer.7.attention.output.dense.bias', 'text_model.encoder.layer.7.attention.output.dense.weight', 'text_model.encoder.layer.7.attention.self.key.bias', 'text_model.encoder.layer.7.attention.self.key.weight', 'text_model.encoder.layer.7.attention.self.query.bias', 'text_model.encoder.layer.7.attention.self.query.weight', 'text_model.encoder.layer.7.attention.self.value.bias', 'text_model.encoder.layer.7.attention.self.value.weight', 'text_model.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.7.crossattention.output.dense.bias', 'text_model.encoder.layer.7.crossattention.output.dense.weight', 'text_model.encoder.layer.7.crossattention.self.key.bias', 'text_model.encoder.layer.7.crossattention.self.key.weight', 'text_model.encoder.layer.7.crossattention.self.query.bias', 'text_model.encoder.layer.7.crossattention.self.query.weight', 'text_model.encoder.layer.7.crossattention.self.value.bias', 'text_model.encoder.layer.7.crossattention.self.value.weight', 'text_model.encoder.layer.7.intermediate.dense.bias', 'text_model.encoder.layer.7.intermediate.dense.weight', 'text_model.encoder.layer.7.output.LayerNorm.bias', 'text_model.encoder.layer.7.output.LayerNorm.weight', 'text_model.encoder.layer.7.output.dense.bias', 'text_model.encoder.layer.7.output.dense.weight', 'text_model.encoder.layer.8.attention.output.LayerNorm.bias', 'text_model.encoder.layer.8.attention.output.LayerNorm.weight', 'text_model.encoder.layer.8.attention.output.dense.bias', 'text_model.encoder.layer.8.attention.output.dense.weight', 'text_model.encoder.layer.8.attention.self.key.bias', 'text_model.encoder.layer.8.attention.self.key.weight', 'text_model.encoder.layer.8.attention.self.query.bias', 'text_model.encoder.layer.8.attention.self.query.weight', 'text_model.encoder.layer.8.attention.self.value.bias', 'text_model.encoder.layer.8.attention.self.value.weight', 'text_model.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.8.crossattention.output.dense.bias', 'text_model.encoder.layer.8.crossattention.output.dense.weight', 'text_model.encoder.layer.8.crossattention.self.key.bias', 'text_model.encoder.layer.8.crossattention.self.key.weight', 'text_model.encoder.layer.8.crossattention.self.query.bias', 'text_model.encoder.layer.8.crossattention.self.query.weight', 'text_model.encoder.layer.8.crossattention.self.value.bias', 'text_model.encoder.layer.8.crossattention.self.value.weight', 'text_model.encoder.layer.8.intermediate.dense.bias', 'text_model.encoder.layer.8.intermediate.dense.weight', 'text_model.encoder.layer.8.output.LayerNorm.bias', 'text_model.encoder.layer.8.output.LayerNorm.weight', 'text_model.encoder.layer.8.output.dense.bias', 'text_model.encoder.layer.8.output.dense.weight', 'text_model.encoder.layer.9.attention.output.LayerNorm.bias', 'text_model.encoder.layer.9.attention.output.LayerNorm.weight', 'text_model.encoder.layer.9.attention.output.dense.bias', 'text_model.encoder.layer.9.attention.output.dense.weight', 'text_model.encoder.layer.9.attention.self.key.bias', 'text_model.encoder.layer.9.attention.self.key.weight', 'text_model.encoder.layer.9.attention.self.query.bias', 'text_model.encoder.layer.9.attention.self.query.weight', 'text_model.encoder.layer.9.attention.self.value.bias', 'text_model.encoder.layer.9.attention.self.value.weight', 'text_model.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.9.crossattention.output.dense.bias', 'text_model.encoder.layer.9.crossattention.output.dense.weight', 'text_model.encoder.layer.9.crossattention.self.key.bias', 'text_model.encoder.layer.9.crossattention.self.key.weight', 'text_model.encoder.layer.9.crossattention.self.query.bias', 'text_model.encoder.layer.9.crossattention.self.query.weight', 'text_model.encoder.layer.9.crossattention.self.value.bias', 'text_model.encoder.layer.9.crossattention.self.value.weight', 'text_model.encoder.layer.9.intermediate.dense.bias', 'text_model.encoder.layer.9.intermediate.dense.weight', 'text_model.encoder.layer.9.output.LayerNorm.bias', 'text_model.encoder.layer.9.output.LayerNorm.weight', 'text_model.encoder.layer.9.output.dense.bias', 'text_model.encoder.layer.9.output.dense.weight', 'text_model.pooler.dense.bias', 'text_model.pooler.dense.weight', 'text_projection.weight', 'visual_projection.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle BLIP chargé avec succès !\n",
      "Extraction des caractéristiques des images d'entraînement...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BlipModel' object has no attribute 'visual_encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 55\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(image_train_folder):\n\u001b[0;32m     54\u001b[0m     image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_train_folder, image_file)\n\u001b[1;32m---> 55\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m extract_image_features(image_path)\n\u001b[0;32m     56\u001b[0m     image_train_features\u001b[38;5;241m.\u001b[39mappend(image_features)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCaractéristiques des images d\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentraînement extraites : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(image_train_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m images.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 44\u001b[0m, in \u001b[0;36mextract_image_features\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     41\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(images\u001b[38;5;241m=\u001b[39mimage, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 44\u001b[0m     features \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mvisual_encoder(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1928\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1926\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1927\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1928\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1929\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1930\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BlipModel' object has no attribute 'visual_encoder'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BlipProcessor, BlipModel, BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 1. Chargement des données\n",
    "print(\"Chargement des données...\")\n",
    "df_Xtrain = pd.read_csv(\"Data/X_train_update.csv\")\n",
    "df_Xtest = pd.read_csv(\"Data/X_test_update.csv\")\n",
    "df_Ytrain = pd.read_csv(\"Data/Y_train.csv\")\n",
    "df_Ytest = pd.read_csv(\"Data/Y_test.csv\")\n",
    "print(\"Données chargées avec succès !\")\n",
    "\n",
    "# 2. Traitement des valeurs manquantes\n",
    "print(\"Traitement des valeurs manquantes...\")\n",
    "df_Xtrain.fillna(0, inplace=True)\n",
    "df_Xtest.fillna(0, inplace=True)\n",
    "df_Ytrain.fillna(0, inplace=True)\n",
    "df_Ytest.fillna(0, inplace=True)\n",
    "print(\"Valeurs manquantes traitées !\")\n",
    "\n",
    "# 3. Fusion des datasets\n",
    "print(\"Fusion des datasets...\")\n",
    "df_train = pd.concat([df_Xtrain, df_Ytrain], axis=1)\n",
    "df_test = pd.concat([df_Xtest, df_Ytest], axis=1)\n",
    "print(\"Fusion terminée !\")\n",
    "\n",
    "# 4. Chargement du modèle et du processeur BLIP pour l'extraction d'images\n",
    "print(\"Chargement du modèle BLIP...\")\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipModel.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model.eval()  # Mode évaluation pour éviter la mise à jour des poids\n",
    "print(\"Modèle BLIP chargé avec succès !\")\n",
    "\n",
    "# 5. Fonction pour extraire les caractéristiques d'une image\n",
    "def extract_image_features(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = model.visual_encoder(**inputs)\n",
    "\n",
    "    return features.squeeze().numpy()\n",
    "\n",
    "# 6. Extraire les caractéristiques des images d'entraînement\n",
    "image_train_folder = \"Data/images/image_train\"\n",
    "image_train_features = []\n",
    "\n",
    "print(\"Extraction des caractéristiques des images d'entraînement...\")\n",
    "for image_file in os.listdir(image_train_folder):\n",
    "    image_path = os.path.join(image_train_folder, image_file)\n",
    "    image_features = extract_image_features(image_path)\n",
    "    image_train_features.append(image_features)\n",
    "print(f\"Caractéristiques des images d'entraînement extraites : {len(image_train_features)} images.\")\n",
    "\n",
    "# 7. Extraire les caractéristiques des images de test\n",
    "image_test_folder = \"Data/images/image_test\"\n",
    "image_test_features = []\n",
    "\n",
    "print(\"Extraction des caractéristiques des images de test...\")\n",
    "for image_file in os.listdir(image_test_folder):\n",
    "    image_path = os.path.join(image_test_folder, image_file)\n",
    "    image_features = extract_image_features(image_path)\n",
    "    image_test_features.append(image_features)\n",
    "print(f\"Caractéristiques des images de test extraites : {len(image_test_features)} images.\")\n",
    "\n",
    "# 8. Chargement du modèle BERT pour l'extraction de caractéristiques textuelles\n",
    "print(\"Chargement du modèle BERT...\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model.eval()\n",
    "print(\"Modèle BERT chargé avec succès !\")\n",
    "\n",
    "# 9. Fonction pour extraire les caractéristiques textuelles avec BERT\n",
    "def extract_text_features(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "\n",
    "# 10. Extraction des caractéristiques des textes\n",
    "print(\"Extraction des caractéristiques des textes...\")\n",
    "text_train_features = [extract_text_features(text) for text in df_train['text_column']]\n",
    "text_test_features = [extract_text_features(text) for text in df_test['text_column']]\n",
    "print(f\"Caractéristiques des textes extraites : {len(text_train_features)} textes.\")\n",
    "\n",
    "# 11. Fusion des caractéristiques images et textes\n",
    "print(\"Fusion des caractéristiques d'images et de texte...\")\n",
    "image_train_features = np.array(image_train_features)\n",
    "image_test_features = np.array(image_test_features)\n",
    "text_train_features = np.array(text_train_features)\n",
    "text_test_features = np.array(text_test_features)\n",
    "\n",
    "X_train_features = np.concatenate([image_train_features, text_train_features], axis=1)\n",
    "X_test_features = np.concatenate([image_test_features, text_test_features], axis=1)\n",
    "print(\"Fusion terminée.\")\n",
    "\n",
    "# 12. Entraînement du modèle RandomForest\n",
    "print(\"Entraînement du modèle RandomForest...\")\n",
    "model_rf = RandomForestClassifier(n_estimators=100)\n",
    "model_rf.fit(X_train_features, df_train['target_column'])\n",
    "\n",
    "# 13. Évaluation du modèle\n",
    "accuracy = model_rf.score(X_test_features, df_test['target_column'])\n",
    "print(f\"Précision du modèle : {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46b8da1-1a0b-47fe-9d60-d8bab11a6759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417fab0b-0d4b-4287-9458-595899c34bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74220030-0ea4-4dab-a57b-d8cdda33befd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des données...\n",
      "Données chargées avec succès !\n",
      "Traitement des valeurs manquantes...\n",
      "Fusion des datasets...\n",
      "Vectorisation des descriptions textuelles...\n",
      "Chargement du modèle CNN pour l'extraction des features...\n",
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
      "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step \n",
      "Extraction des features pour 84916 images...\n",
      "Extraction des features pour 13812 images...\n",
      "Réduction de dimension des features d'images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a740407\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:685: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = self.explained_variance_ / total_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion des features texte et image...\n",
      "Création du pipeline et début de l'entraînement...\n",
      "Modèle entraîné en 757.33 secondes.\n",
      "Prédiction et évaluation du modèle...\n",
      "Précision du modèle : 0.0036\n",
      "Fin du processus ! 🎯\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# ✅ Chargement des données\n",
    "print(\"Chargement des données...\")\n",
    "df_Xtrain = pd.read_csv(\"Data/X_train_update.csv\")\n",
    "df_Xtest = pd.read_csv(\"Data/X_test_update.csv\")\n",
    "df_Ytrain = pd.read_csv(\"Data/Y_train.csv\")\n",
    "df_Ytest = pd.read_csv(\"Data/Y_test.csv\")\n",
    "print(\"Données chargées avec succès !\")\n",
    "\n",
    "# ✅ Traitement des valeurs manquantes\n",
    "print(\"Traitement des valeurs manquantes...\")\n",
    "df_Xtrain['description'] = df_Xtrain['description'].fillna(\"aucune description\")\n",
    "df_Xtest['description'] = df_Xtest['description'].fillna(\"aucune description\")\n",
    "\n",
    "# ✅ Fusion des features avec les labels\n",
    "print(\"Fusion des datasets...\")\n",
    "df_train = df_Xtrain.merge(df_Ytrain, on=\"Unnamed: 0\").drop(columns=[\"Unnamed: 0\"])\n",
    "df_test = df_Xtest.merge(df_Ytest, on=\"Unnamed: 0\").drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "# ✅ Extraction des features textuelles\n",
    "print(\"Vectorisation des descriptions textuelles...\")\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train_text = vectorizer.fit_transform(df_train[\"description\"]).toarray()\n",
    "X_test_text = vectorizer.transform(df_test[\"description\"]).toarray()\n",
    "\n",
    "# ✅ Chargement du modèle de feature extraction d'images\n",
    "print(\"Chargement du modèle CNN pour l'extraction des features...\")\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, pooling='avg')\n",
    "model = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "# ✅ Fonction d'extraction des features d'une image\n",
    "def extract_image_features(image_path):\n",
    "    try:\n",
    "        img = image.load_img(image_path, target_size=(224, 224))  \n",
    "        img_array = image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)  \n",
    "        img_array = preprocess_input(img_array)  \n",
    "        features = model.predict(img_array)  \n",
    "        return features.flatten()\n",
    "    except:\n",
    "        return np.zeros((1280,))  # Retourne un vecteur nul si l'image est absente ou corrompue\n",
    "\n",
    "# ✅ Chargement des features des images\n",
    "def get_image_features(df, image_folder):\n",
    "    print(f\"Extraction des features pour {len(df)} images...\")\n",
    "    image_features = []\n",
    "    for img_id in df[\"imageid\"]:\n",
    "        img_path = os.path.join(image_folder, f\"{img_id}.jpg\")\n",
    "        image_features.append(extract_image_features(img_path))\n",
    "    return np.array(image_features)\n",
    "\n",
    "# ✅ Extraction des features d'images\n",
    "X_train_img = get_image_features(df_train, \"Data/images/image_train\")\n",
    "X_test_img = get_image_features(df_test, \"Data/images/image_test\")\n",
    "\n",
    "# ✅ Réduction de dimension des features d'images (optionnel)\n",
    "print(\"Réduction de dimension des features d'images...\")\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=256)  # On réduit à 256 dimensions\n",
    "X_train_img = pca.fit_transform(scaler.fit_transform(X_train_img))\n",
    "X_test_img = pca.transform(scaler.transform(X_test_img))\n",
    "\n",
    "# ✅ Fusion des features textuelles et visuelles\n",
    "print(\"Fusion des features texte et image...\")\n",
    "X_train = np.hstack((X_train_text, X_train_img))\n",
    "X_test = np.hstack((X_test_text, X_test_img))\n",
    "\n",
    "# ✅ Définition du modèle de classification\n",
    "print(\"Création du pipeline et début de l'entraînement...\")\n",
    "classifier = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "\n",
    "# ✅ Entraînement du modèle\n",
    "start_time = time.time()\n",
    "classifier.fit(X_train, df_train[\"prdtypecode\"])\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Modèle entraîné en {training_time:.2f} secondes.\")\n",
    "\n",
    "# ✅ Prédictions et évaluation\n",
    "print(\"Prédiction et évaluation du modèle...\")\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(df_test[\"prdtypecode\"], y_pred)\n",
    "print(f\"Précision du modèle : {accuracy:.4f}\")\n",
    "\n",
    "print(\"Fin du processus ! 🎯\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57664527-4c50-436a-9bd6-e715f967ebd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bfddee-ffbd-44ee-8c95-6b83067d3cac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac08ec11-537c-4561-817a-5f3d1a821d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4183cf08-8d2f-4264-a54c-3ac826b7caaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61af898-2f04-48a1-9f46-39231f985204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c66b3e-27ee-4b9d-890c-444a232c92c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aab6c16-266e-4b8d-8aef-70e6d2d8edc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46ed733-a379-4ca3-a321-6fdd95339d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4844e853-7af7-4219-afd0-ff6690621df9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7802a3-28b3-4450-bd4d-77181d4fb0c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afe463e-8631-4e21-82e1-a03c6818b666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c503f470-75b5-4ffe-9edc-22f755badb7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8b4ad8-fb15-4a3d-9f4f-55e74726e6bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b0a682-5884-4396-b9d3-6bd76aff9d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c069d85d-3a90-4aa4-8e0f-eb105a16b598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3546482c-39a3-42bf-ba13-5eedc902d969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f43ed1-000c-442d-98ce-2da1a6ae172a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa056759-1644-4779-8d69-db8b4a3116bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f0d85a-10f7-4fe3-8362-da284e5cbfab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc4e3e1-d04a-4f95-8434-a1f9185e08c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b944c0-3c94-4211-8b9c-5d7d7f84d0d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6db2a1-71fb-466d-83b5-a83b2e112220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
