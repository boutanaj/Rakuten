{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25ae6451-39c9-40ab-ad19-9cab9c34d73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32df7bac-0fc0-427f-8ef1-cb1396dbfbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Xtrain = pd.read_csv(\"Data/x_train_update.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eb64e81-6bdb-4c71-85e7-ab0c5ab0320e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3804725264</td>\n",
       "      <td>1263597046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Journal Des Arts (Le) NÂ° 133 Du 28/09/2001 - L...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>436067568</td>\n",
       "      <td>1008141237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n",
       "      <td>PILOT STYLE Touch Pen de marque Speedlink est ...</td>\n",
       "      <td>201115110</td>\n",
       "      <td>938777978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Peluche Donald - Europe - Disneyland 2000 (Mar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50418756</td>\n",
       "      <td>457047496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>La Guerre Des Tuques</td>\n",
       "      <td>Luc a des id&amp;eacute;es de grandeur. Il veut or...</td>\n",
       "      <td>278535884</td>\n",
       "      <td>1077757786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        designation  \\\n",
       "0           0  Olivia: Personalisiertes Notizbuch / 150 Seite...   \n",
       "1           1  Journal Des Arts (Le) NÂ° 133 Du 28/09/2001 - L...   \n",
       "2           2  Grand Stylet Ergonomique Bleu Gamepad Nintendo...   \n",
       "3           3  Peluche Donald - Europe - Disneyland 2000 (Mar...   \n",
       "4           4                               La Guerre Des Tuques   \n",
       "\n",
       "                                         description   productid     imageid  \n",
       "0                                                NaN  3804725264  1263597046  \n",
       "1                                                NaN   436067568  1008141237  \n",
       "2  PILOT STYLE Touch Pen de marque Speedlink est ...   201115110   938777978  \n",
       "3                                                NaN    50418756   457047496  \n",
       "4  Luc a des id&eacute;es de grandeur. Il veut or...   278535884  1077757786  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Xtrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d05a8c50-7abb-437a-93a3-89ec963075b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84916, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a4c5ab8-b35a-4c14-9d00-53bde78339db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0         0\n",
       "designation        0\n",
       "description    29800\n",
       "productid          0\n",
       "imageid            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Xtrain.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56942308-e8ae-4c1d-8279-75a81bcc83ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727e8cc5-ae1c-4e05-b4fb-14b6a30556ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4f1963-22ad-4e96-b12e-8eb22b4701fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c35e7e4-0bde-4688-93b3-9a12bb25278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Xtest = pd.read_csv(\"Data/x_test_update.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe120fca-1552-4590-830d-51bc290c570c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84916</td>\n",
       "      <td>Folkmanis Puppets - 2732 - Marionnette Et ThÃ©Ã¢...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>516376098</td>\n",
       "      <td>1019294171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84917</td>\n",
       "      <td>Porte Flamme Gaxix - Flamebringer Gaxix - 136/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>133389013</td>\n",
       "      <td>1274228667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84918</td>\n",
       "      <td>Pompe de filtration Speck Badu 95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4128438366</td>\n",
       "      <td>1295960357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84919</td>\n",
       "      <td>Robot de piscine Ã©lectrique</td>\n",
       "      <td>&lt;p&gt;Ce robot de piscine d&amp;#39;un design innovan...</td>\n",
       "      <td>3929899732</td>\n",
       "      <td>1265224052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84920</td>\n",
       "      <td>Hsm Destructeur Securio C16 Coupe CroisÂ¿E: 4 X...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>152993898</td>\n",
       "      <td>940543690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        designation  \\\n",
       "0       84916  Folkmanis Puppets - 2732 - Marionnette Et ThÃ©Ã¢...   \n",
       "1       84917  Porte Flamme Gaxix - Flamebringer Gaxix - 136/...   \n",
       "2       84918                  Pompe de filtration Speck Badu 95   \n",
       "3       84919                        Robot de piscine Ã©lectrique   \n",
       "4       84920  Hsm Destructeur Securio C16 Coupe CroisÂ¿E: 4 X...   \n",
       "\n",
       "                                         description   productid     imageid  \n",
       "0                                                NaN   516376098  1019294171  \n",
       "1                                                NaN   133389013  1274228667  \n",
       "2                                                NaN  4128438366  1295960357  \n",
       "3  <p>Ce robot de piscine d&#39;un design innovan...  3929899732  1265224052  \n",
       "4                                                NaN   152993898   940543690  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Xtest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9af7404-23d5-485d-a06f-3d0a53d42b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13812, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95888f8d-3110-4599-a271-30367c8bbe3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0        0\n",
       "designation       0\n",
       "description    4886\n",
       "productid         0\n",
       "imageid           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Xtest.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa317c7a-eddc-4cf2-884f-572ba321c616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "068563b7-6ed6-4f9a-94b7-fff512b770f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Ytrain = pd.read_csv(\"Data/Y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83067b10-8d2e-4027-92e1-d4e73c07e0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Ytest = pd.read_csv(\"Data/Y_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46213650-e621-4137-974a-a3e6a622f0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13812, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1515a4a5-fff0-4ba9-817f-c794b93425bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>prdtypecode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84916</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84917</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84918</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84919</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84920</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  prdtypecode\n",
       "0       84916           10\n",
       "1       84917           10\n",
       "2       84918           10\n",
       "3       84919           10\n",
       "4       84920           10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Ytrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5907bb44-5970-4d6d-8622-9c14d026315f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0     0\n",
       "prdtypecode    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Ytrain.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eaf241-b9f7-4c67-878e-7885021c598d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd91d3be-3542-4e7d-9b21-5e83210e6237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84916, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1673a09-561f-40d4-83a5-3186286c85a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>prdtypecode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  prdtypecode\n",
       "0           0           10\n",
       "1           1         2280\n",
       "2           2           50\n",
       "3           3         1280\n",
       "4           4         2705"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Ytest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52495f29-3f1b-478e-9567-c6fa7c5d18a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0     0\n",
       "prdtypecode    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Ytest.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ac87da-2dd4-448e-8a18-cfd717ea676c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755d03fa-b64e-48db-9059-b48dce77544c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20b732d1-cd4e-4837-994c-630ef3244c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_Xtrain_image_sample1 = pd.read_csv(\"Data/images/image_train/image_528113_product_923222.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb3a6b-d0f2-4b35-9983-c78f7a8e6695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8cce237-deeb-4a39-854b-5368e0df4e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'designation', 'description', 'productid', 'imageid'], dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Xtrain.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31484a20-aca7-446c-8d58-ed2693b3b63e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'designation', 'description', 'productid', 'imageid'], dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Xtest.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8569776d-687a-466e-ac13-1c5d164252a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'prdtypecode'], dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Ytrain.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e20d6840-5429-4712-850b-47fcead1545e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'prdtypecode'], dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Ytest.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536fbf9a-b37f-4462-a0bf-67ea33b1b815",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac2d85a-ab95-4486-8e7a-c35eccf9ae2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des donnÃ©es...\n",
      "DonnÃ©es chargÃ©es avec succÃ¨s !\n",
      "Traitement des valeurs manquantes...\n",
      "Valeurs manquantes traitÃ©es !\n",
      "Fusion des datasets...\n",
      "Fusion terminÃ©e !\n",
      "CrÃ©ation du pipeline et dÃ©but de l'entraÃ®nement...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# âœ… Chargement des donnÃ©es\n",
    "print(\"Chargement des donnÃ©es...\")\n",
    "df_Xtrain = pd.read_csv(\"Data/X_train_update.csv\")\n",
    "df_Xtest = pd.read_csv(\"Data/X_test_update.csv\")\n",
    "df_Ytrain = pd.read_csv(\"Data/Y_train.csv\")\n",
    "df_Ytest = pd.read_csv(\"Data/Y_test.csv\")\n",
    "print(\"DonnÃ©es chargÃ©es avec succÃ¨s !\")\n",
    "\n",
    "# âœ… Correction du warning en Ã©vitant inplace=True\n",
    "print(\"Traitement des valeurs manquantes...\")\n",
    "df_Xtrain = df_Xtrain.copy()\n",
    "df_Xtest = df_Xtest.copy()\n",
    "\n",
    "df_Xtrain['description'] = df_Xtrain['description'].fillna(\"aucune description\")\n",
    "df_Xtest['description'] = df_Xtest['description'].fillna(\"aucune description\")\n",
    "print(\"Valeurs manquantes traitÃ©es !\")\n",
    "\n",
    "# âœ… Fusion des features avec les labels\n",
    "print(\"Fusion des datasets...\")\n",
    "df_train = df_Xtrain.merge(df_Ytrain, on=\"Unnamed: 0\").drop(columns=[\"Unnamed: 0\"])\n",
    "df_test = df_Xtest.merge(df_Ytest, on=\"Unnamed: 0\").drop(columns=[\"Unnamed: 0\"])\n",
    "print(\"Fusion terminÃ©e !\")\n",
    "\n",
    "# âœ… CrÃ©ation des variables X (features) et y (labels)\n",
    "X_train_text = df_train[\"description\"]\n",
    "X_test_text = df_test[\"description\"]\n",
    "y_train = df_train[\"prdtypecode\"]\n",
    "y_test = df_test[\"prdtypecode\"]\n",
    "\n",
    "# âœ… CrÃ©ation d'un pipeline avec un vectoriseur TF-IDF et un modÃ¨le de classification\n",
    "print(\"CrÃ©ation du pipeline et dÃ©but de l'entraÃ®nement...\")\n",
    "pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(stop_words=None, max_features=10000)),  # Vectorisation des descriptions\n",
    "    (\"classifier\", RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1))  # ModÃ¨le\n",
    "])\n",
    "\n",
    "#from nltk.corpus import stopwords\n",
    "#stop_words = stopwords.words('french')\n",
    "#vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "# âœ… Mesure du temps d'entraÃ®nement\n",
    "start_time = time.time()\n",
    "pipeline.fit(X_train_text, y_train)\n",
    "training_time = time.time() - start_time\n",
    "print(f\"ModÃ¨le entraÃ®nÃ© en {training_time:.2f} secondes.\")\n",
    "\n",
    "# âœ… PrÃ©dictions et Ã©valuation\n",
    "print(\"PrÃ©diction et Ã©valuation du modÃ¨le...\")\n",
    "y_pred = pipeline.predict(X_test_text)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"PrÃ©cision du modÃ¨le : {accuracy:.4f}\")\n",
    "\n",
    "print(\"Fin du processus ! ðŸŽ¯\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ba2554-01b0-4d19-a604-de578c9efc5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e41911c-6d1b-454e-b205-bb0ca543f1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision transformers Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c94e5c3-33c3-45a3-bd7d-f4b4bd9a6cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92adb20e-4c3d-4a31-8db4-5a7ba4bd2d37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5e608d-a25d-480e-923f-ff55d1c0f437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651d8d08-b25c-4307-9783-faae8946989a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64d6ee9-67c5-408e-9315-16703266a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traintement des donnÃ©es textuels et visuels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5820b880-56af-41bc-8078-a6d628847766",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade optree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2be7b0-216a-404e-a85e-381c0de3fe3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285d8faf-515f-40af-ac83-ddc8878a4596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4f02e4-3e69-4708-b866-d7bc923d88fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dd94d8-c008-4c57-8692-15e6bd01b4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68dc3eee-e5a4-445c-b31d-2ab0f6413d70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des donnÃ©es...\n",
      "DonnÃ©es chargÃ©es avec succÃ¨s !\n",
      "Traitement des valeurs manquantes...\n",
      "Valeurs manquantes traitÃ©es !\n",
      "Fusion des datasets...\n",
      "Fusion terminÃ©e !\n",
      "Chargement du modÃ¨le BLIP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a740407\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "`BlipModel` is going to be deprecated in future release, please use `BlipForConditionalGeneration`, `BlipForQuestionAnswering` or `BlipForImageTextRetrieval` depending on your usecase.\n",
      "Some weights of BlipModel were not initialized from the model checkpoint at Salesforce/blip-image-captioning-base and are newly initialized: ['logit_scale', 'text_model.embeddings.LayerNorm.bias', 'text_model.embeddings.LayerNorm.weight', 'text_model.embeddings.position_embeddings.weight', 'text_model.embeddings.word_embeddings.weight', 'text_model.encoder.layer.0.attention.output.LayerNorm.bias', 'text_model.encoder.layer.0.attention.output.LayerNorm.weight', 'text_model.encoder.layer.0.attention.output.dense.bias', 'text_model.encoder.layer.0.attention.output.dense.weight', 'text_model.encoder.layer.0.attention.self.key.bias', 'text_model.encoder.layer.0.attention.self.key.weight', 'text_model.encoder.layer.0.attention.self.query.bias', 'text_model.encoder.layer.0.attention.self.query.weight', 'text_model.encoder.layer.0.attention.self.value.bias', 'text_model.encoder.layer.0.attention.self.value.weight', 'text_model.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.0.crossattention.output.dense.bias', 'text_model.encoder.layer.0.crossattention.output.dense.weight', 'text_model.encoder.layer.0.crossattention.self.key.bias', 'text_model.encoder.layer.0.crossattention.self.key.weight', 'text_model.encoder.layer.0.crossattention.self.query.bias', 'text_model.encoder.layer.0.crossattention.self.query.weight', 'text_model.encoder.layer.0.crossattention.self.value.bias', 'text_model.encoder.layer.0.crossattention.self.value.weight', 'text_model.encoder.layer.0.intermediate.dense.bias', 'text_model.encoder.layer.0.intermediate.dense.weight', 'text_model.encoder.layer.0.output.LayerNorm.bias', 'text_model.encoder.layer.0.output.LayerNorm.weight', 'text_model.encoder.layer.0.output.dense.bias', 'text_model.encoder.layer.0.output.dense.weight', 'text_model.encoder.layer.1.attention.output.LayerNorm.bias', 'text_model.encoder.layer.1.attention.output.LayerNorm.weight', 'text_model.encoder.layer.1.attention.output.dense.bias', 'text_model.encoder.layer.1.attention.output.dense.weight', 'text_model.encoder.layer.1.attention.self.key.bias', 'text_model.encoder.layer.1.attention.self.key.weight', 'text_model.encoder.layer.1.attention.self.query.bias', 'text_model.encoder.layer.1.attention.self.query.weight', 'text_model.encoder.layer.1.attention.self.value.bias', 'text_model.encoder.layer.1.attention.self.value.weight', 'text_model.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.1.crossattention.output.dense.bias', 'text_model.encoder.layer.1.crossattention.output.dense.weight', 'text_model.encoder.layer.1.crossattention.self.key.bias', 'text_model.encoder.layer.1.crossattention.self.key.weight', 'text_model.encoder.layer.1.crossattention.self.query.bias', 'text_model.encoder.layer.1.crossattention.self.query.weight', 'text_model.encoder.layer.1.crossattention.self.value.bias', 'text_model.encoder.layer.1.crossattention.self.value.weight', 'text_model.encoder.layer.1.intermediate.dense.bias', 'text_model.encoder.layer.1.intermediate.dense.weight', 'text_model.encoder.layer.1.output.LayerNorm.bias', 'text_model.encoder.layer.1.output.LayerNorm.weight', 'text_model.encoder.layer.1.output.dense.bias', 'text_model.encoder.layer.1.output.dense.weight', 'text_model.encoder.layer.10.attention.output.LayerNorm.bias', 'text_model.encoder.layer.10.attention.output.LayerNorm.weight', 'text_model.encoder.layer.10.attention.output.dense.bias', 'text_model.encoder.layer.10.attention.output.dense.weight', 'text_model.encoder.layer.10.attention.self.key.bias', 'text_model.encoder.layer.10.attention.self.key.weight', 'text_model.encoder.layer.10.attention.self.query.bias', 'text_model.encoder.layer.10.attention.self.query.weight', 'text_model.encoder.layer.10.attention.self.value.bias', 'text_model.encoder.layer.10.attention.self.value.weight', 'text_model.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.10.crossattention.output.dense.bias', 'text_model.encoder.layer.10.crossattention.output.dense.weight', 'text_model.encoder.layer.10.crossattention.self.key.bias', 'text_model.encoder.layer.10.crossattention.self.key.weight', 'text_model.encoder.layer.10.crossattention.self.query.bias', 'text_model.encoder.layer.10.crossattention.self.query.weight', 'text_model.encoder.layer.10.crossattention.self.value.bias', 'text_model.encoder.layer.10.crossattention.self.value.weight', 'text_model.encoder.layer.10.intermediate.dense.bias', 'text_model.encoder.layer.10.intermediate.dense.weight', 'text_model.encoder.layer.10.output.LayerNorm.bias', 'text_model.encoder.layer.10.output.LayerNorm.weight', 'text_model.encoder.layer.10.output.dense.bias', 'text_model.encoder.layer.10.output.dense.weight', 'text_model.encoder.layer.11.attention.output.LayerNorm.bias', 'text_model.encoder.layer.11.attention.output.LayerNorm.weight', 'text_model.encoder.layer.11.attention.output.dense.bias', 'text_model.encoder.layer.11.attention.output.dense.weight', 'text_model.encoder.layer.11.attention.self.key.bias', 'text_model.encoder.layer.11.attention.self.key.weight', 'text_model.encoder.layer.11.attention.self.query.bias', 'text_model.encoder.layer.11.attention.self.query.weight', 'text_model.encoder.layer.11.attention.self.value.bias', 'text_model.encoder.layer.11.attention.self.value.weight', 'text_model.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.11.crossattention.output.dense.bias', 'text_model.encoder.layer.11.crossattention.output.dense.weight', 'text_model.encoder.layer.11.crossattention.self.key.bias', 'text_model.encoder.layer.11.crossattention.self.key.weight', 'text_model.encoder.layer.11.crossattention.self.query.bias', 'text_model.encoder.layer.11.crossattention.self.query.weight', 'text_model.encoder.layer.11.crossattention.self.value.bias', 'text_model.encoder.layer.11.crossattention.self.value.weight', 'text_model.encoder.layer.11.intermediate.dense.bias', 'text_model.encoder.layer.11.intermediate.dense.weight', 'text_model.encoder.layer.11.output.LayerNorm.bias', 'text_model.encoder.layer.11.output.LayerNorm.weight', 'text_model.encoder.layer.11.output.dense.bias', 'text_model.encoder.layer.11.output.dense.weight', 'text_model.encoder.layer.2.attention.output.LayerNorm.bias', 'text_model.encoder.layer.2.attention.output.LayerNorm.weight', 'text_model.encoder.layer.2.attention.output.dense.bias', 'text_model.encoder.layer.2.attention.output.dense.weight', 'text_model.encoder.layer.2.attention.self.key.bias', 'text_model.encoder.layer.2.attention.self.key.weight', 'text_model.encoder.layer.2.attention.self.query.bias', 'text_model.encoder.layer.2.attention.self.query.weight', 'text_model.encoder.layer.2.attention.self.value.bias', 'text_model.encoder.layer.2.attention.self.value.weight', 'text_model.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.2.crossattention.output.dense.bias', 'text_model.encoder.layer.2.crossattention.output.dense.weight', 'text_model.encoder.layer.2.crossattention.self.key.bias', 'text_model.encoder.layer.2.crossattention.self.key.weight', 'text_model.encoder.layer.2.crossattention.self.query.bias', 'text_model.encoder.layer.2.crossattention.self.query.weight', 'text_model.encoder.layer.2.crossattention.self.value.bias', 'text_model.encoder.layer.2.crossattention.self.value.weight', 'text_model.encoder.layer.2.intermediate.dense.bias', 'text_model.encoder.layer.2.intermediate.dense.weight', 'text_model.encoder.layer.2.output.LayerNorm.bias', 'text_model.encoder.layer.2.output.LayerNorm.weight', 'text_model.encoder.layer.2.output.dense.bias', 'text_model.encoder.layer.2.output.dense.weight', 'text_model.encoder.layer.3.attention.output.LayerNorm.bias', 'text_model.encoder.layer.3.attention.output.LayerNorm.weight', 'text_model.encoder.layer.3.attention.output.dense.bias', 'text_model.encoder.layer.3.attention.output.dense.weight', 'text_model.encoder.layer.3.attention.self.key.bias', 'text_model.encoder.layer.3.attention.self.key.weight', 'text_model.encoder.layer.3.attention.self.query.bias', 'text_model.encoder.layer.3.attention.self.query.weight', 'text_model.encoder.layer.3.attention.self.value.bias', 'text_model.encoder.layer.3.attention.self.value.weight', 'text_model.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.3.crossattention.output.dense.bias', 'text_model.encoder.layer.3.crossattention.output.dense.weight', 'text_model.encoder.layer.3.crossattention.self.key.bias', 'text_model.encoder.layer.3.crossattention.self.key.weight', 'text_model.encoder.layer.3.crossattention.self.query.bias', 'text_model.encoder.layer.3.crossattention.self.query.weight', 'text_model.encoder.layer.3.crossattention.self.value.bias', 'text_model.encoder.layer.3.crossattention.self.value.weight', 'text_model.encoder.layer.3.intermediate.dense.bias', 'text_model.encoder.layer.3.intermediate.dense.weight', 'text_model.encoder.layer.3.output.LayerNorm.bias', 'text_model.encoder.layer.3.output.LayerNorm.weight', 'text_model.encoder.layer.3.output.dense.bias', 'text_model.encoder.layer.3.output.dense.weight', 'text_model.encoder.layer.4.attention.output.LayerNorm.bias', 'text_model.encoder.layer.4.attention.output.LayerNorm.weight', 'text_model.encoder.layer.4.attention.output.dense.bias', 'text_model.encoder.layer.4.attention.output.dense.weight', 'text_model.encoder.layer.4.attention.self.key.bias', 'text_model.encoder.layer.4.attention.self.key.weight', 'text_model.encoder.layer.4.attention.self.query.bias', 'text_model.encoder.layer.4.attention.self.query.weight', 'text_model.encoder.layer.4.attention.self.value.bias', 'text_model.encoder.layer.4.attention.self.value.weight', 'text_model.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.4.crossattention.output.dense.bias', 'text_model.encoder.layer.4.crossattention.output.dense.weight', 'text_model.encoder.layer.4.crossattention.self.key.bias', 'text_model.encoder.layer.4.crossattention.self.key.weight', 'text_model.encoder.layer.4.crossattention.self.query.bias', 'text_model.encoder.layer.4.crossattention.self.query.weight', 'text_model.encoder.layer.4.crossattention.self.value.bias', 'text_model.encoder.layer.4.crossattention.self.value.weight', 'text_model.encoder.layer.4.intermediate.dense.bias', 'text_model.encoder.layer.4.intermediate.dense.weight', 'text_model.encoder.layer.4.output.LayerNorm.bias', 'text_model.encoder.layer.4.output.LayerNorm.weight', 'text_model.encoder.layer.4.output.dense.bias', 'text_model.encoder.layer.4.output.dense.weight', 'text_model.encoder.layer.5.attention.output.LayerNorm.bias', 'text_model.encoder.layer.5.attention.output.LayerNorm.weight', 'text_model.encoder.layer.5.attention.output.dense.bias', 'text_model.encoder.layer.5.attention.output.dense.weight', 'text_model.encoder.layer.5.attention.self.key.bias', 'text_model.encoder.layer.5.attention.self.key.weight', 'text_model.encoder.layer.5.attention.self.query.bias', 'text_model.encoder.layer.5.attention.self.query.weight', 'text_model.encoder.layer.5.attention.self.value.bias', 'text_model.encoder.layer.5.attention.self.value.weight', 'text_model.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.5.crossattention.output.dense.bias', 'text_model.encoder.layer.5.crossattention.output.dense.weight', 'text_model.encoder.layer.5.crossattention.self.key.bias', 'text_model.encoder.layer.5.crossattention.self.key.weight', 'text_model.encoder.layer.5.crossattention.self.query.bias', 'text_model.encoder.layer.5.crossattention.self.query.weight', 'text_model.encoder.layer.5.crossattention.self.value.bias', 'text_model.encoder.layer.5.crossattention.self.value.weight', 'text_model.encoder.layer.5.intermediate.dense.bias', 'text_model.encoder.layer.5.intermediate.dense.weight', 'text_model.encoder.layer.5.output.LayerNorm.bias', 'text_model.encoder.layer.5.output.LayerNorm.weight', 'text_model.encoder.layer.5.output.dense.bias', 'text_model.encoder.layer.5.output.dense.weight', 'text_model.encoder.layer.6.attention.output.LayerNorm.bias', 'text_model.encoder.layer.6.attention.output.LayerNorm.weight', 'text_model.encoder.layer.6.attention.output.dense.bias', 'text_model.encoder.layer.6.attention.output.dense.weight', 'text_model.encoder.layer.6.attention.self.key.bias', 'text_model.encoder.layer.6.attention.self.key.weight', 'text_model.encoder.layer.6.attention.self.query.bias', 'text_model.encoder.layer.6.attention.self.query.weight', 'text_model.encoder.layer.6.attention.self.value.bias', 'text_model.encoder.layer.6.attention.self.value.weight', 'text_model.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.6.crossattention.output.dense.bias', 'text_model.encoder.layer.6.crossattention.output.dense.weight', 'text_model.encoder.layer.6.crossattention.self.key.bias', 'text_model.encoder.layer.6.crossattention.self.key.weight', 'text_model.encoder.layer.6.crossattention.self.query.bias', 'text_model.encoder.layer.6.crossattention.self.query.weight', 'text_model.encoder.layer.6.crossattention.self.value.bias', 'text_model.encoder.layer.6.crossattention.self.value.weight', 'text_model.encoder.layer.6.intermediate.dense.bias', 'text_model.encoder.layer.6.intermediate.dense.weight', 'text_model.encoder.layer.6.output.LayerNorm.bias', 'text_model.encoder.layer.6.output.LayerNorm.weight', 'text_model.encoder.layer.6.output.dense.bias', 'text_model.encoder.layer.6.output.dense.weight', 'text_model.encoder.layer.7.attention.output.LayerNorm.bias', 'text_model.encoder.layer.7.attention.output.LayerNorm.weight', 'text_model.encoder.layer.7.attention.output.dense.bias', 'text_model.encoder.layer.7.attention.output.dense.weight', 'text_model.encoder.layer.7.attention.self.key.bias', 'text_model.encoder.layer.7.attention.self.key.weight', 'text_model.encoder.layer.7.attention.self.query.bias', 'text_model.encoder.layer.7.attention.self.query.weight', 'text_model.encoder.layer.7.attention.self.value.bias', 'text_model.encoder.layer.7.attention.self.value.weight', 'text_model.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.7.crossattention.output.dense.bias', 'text_model.encoder.layer.7.crossattention.output.dense.weight', 'text_model.encoder.layer.7.crossattention.self.key.bias', 'text_model.encoder.layer.7.crossattention.self.key.weight', 'text_model.encoder.layer.7.crossattention.self.query.bias', 'text_model.encoder.layer.7.crossattention.self.query.weight', 'text_model.encoder.layer.7.crossattention.self.value.bias', 'text_model.encoder.layer.7.crossattention.self.value.weight', 'text_model.encoder.layer.7.intermediate.dense.bias', 'text_model.encoder.layer.7.intermediate.dense.weight', 'text_model.encoder.layer.7.output.LayerNorm.bias', 'text_model.encoder.layer.7.output.LayerNorm.weight', 'text_model.encoder.layer.7.output.dense.bias', 'text_model.encoder.layer.7.output.dense.weight', 'text_model.encoder.layer.8.attention.output.LayerNorm.bias', 'text_model.encoder.layer.8.attention.output.LayerNorm.weight', 'text_model.encoder.layer.8.attention.output.dense.bias', 'text_model.encoder.layer.8.attention.output.dense.weight', 'text_model.encoder.layer.8.attention.self.key.bias', 'text_model.encoder.layer.8.attention.self.key.weight', 'text_model.encoder.layer.8.attention.self.query.bias', 'text_model.encoder.layer.8.attention.self.query.weight', 'text_model.encoder.layer.8.attention.self.value.bias', 'text_model.encoder.layer.8.attention.self.value.weight', 'text_model.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.8.crossattention.output.dense.bias', 'text_model.encoder.layer.8.crossattention.output.dense.weight', 'text_model.encoder.layer.8.crossattention.self.key.bias', 'text_model.encoder.layer.8.crossattention.self.key.weight', 'text_model.encoder.layer.8.crossattention.self.query.bias', 'text_model.encoder.layer.8.crossattention.self.query.weight', 'text_model.encoder.layer.8.crossattention.self.value.bias', 'text_model.encoder.layer.8.crossattention.self.value.weight', 'text_model.encoder.layer.8.intermediate.dense.bias', 'text_model.encoder.layer.8.intermediate.dense.weight', 'text_model.encoder.layer.8.output.LayerNorm.bias', 'text_model.encoder.layer.8.output.LayerNorm.weight', 'text_model.encoder.layer.8.output.dense.bias', 'text_model.encoder.layer.8.output.dense.weight', 'text_model.encoder.layer.9.attention.output.LayerNorm.bias', 'text_model.encoder.layer.9.attention.output.LayerNorm.weight', 'text_model.encoder.layer.9.attention.output.dense.bias', 'text_model.encoder.layer.9.attention.output.dense.weight', 'text_model.encoder.layer.9.attention.self.key.bias', 'text_model.encoder.layer.9.attention.self.key.weight', 'text_model.encoder.layer.9.attention.self.query.bias', 'text_model.encoder.layer.9.attention.self.query.weight', 'text_model.encoder.layer.9.attention.self.value.bias', 'text_model.encoder.layer.9.attention.self.value.weight', 'text_model.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.9.crossattention.output.dense.bias', 'text_model.encoder.layer.9.crossattention.output.dense.weight', 'text_model.encoder.layer.9.crossattention.self.key.bias', 'text_model.encoder.layer.9.crossattention.self.key.weight', 'text_model.encoder.layer.9.crossattention.self.query.bias', 'text_model.encoder.layer.9.crossattention.self.query.weight', 'text_model.encoder.layer.9.crossattention.self.value.bias', 'text_model.encoder.layer.9.crossattention.self.value.weight', 'text_model.encoder.layer.9.intermediate.dense.bias', 'text_model.encoder.layer.9.intermediate.dense.weight', 'text_model.encoder.layer.9.output.LayerNorm.bias', 'text_model.encoder.layer.9.output.LayerNorm.weight', 'text_model.encoder.layer.9.output.dense.bias', 'text_model.encoder.layer.9.output.dense.weight', 'text_model.pooler.dense.bias', 'text_model.pooler.dense.weight', 'text_projection.weight', 'visual_projection.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModÃ¨le BLIP chargÃ© avec succÃ¨s !\n",
      "Extraction des caractÃ©ristiques des images d'entraÃ®nement...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BlipModel' object has no attribute 'visual_encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 55\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(image_train_folder):\n\u001b[0;32m     54\u001b[0m     image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_train_folder, image_file)\n\u001b[1;32m---> 55\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m extract_image_features(image_path)\n\u001b[0;32m     56\u001b[0m     image_train_features\u001b[38;5;241m.\u001b[39mappend(image_features)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCaractÃ©ristiques des images d\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentraÃ®nement extraites : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(image_train_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m images.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 44\u001b[0m, in \u001b[0;36mextract_image_features\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     41\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(images\u001b[38;5;241m=\u001b[39mimage, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 44\u001b[0m     features \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mvisual_encoder(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1928\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1926\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1927\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1928\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1929\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1930\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BlipModel' object has no attribute 'visual_encoder'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BlipProcessor, BlipModel, BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 1. Chargement des donnÃ©es\n",
    "print(\"Chargement des donnÃ©es...\")\n",
    "df_Xtrain = pd.read_csv(\"Data/X_train_update.csv\")\n",
    "df_Xtest = pd.read_csv(\"Data/X_test_update.csv\")\n",
    "df_Ytrain = pd.read_csv(\"Data/Y_train.csv\")\n",
    "df_Ytest = pd.read_csv(\"Data/Y_test.csv\")\n",
    "print(\"DonnÃ©es chargÃ©es avec succÃ¨s !\")\n",
    "\n",
    "# 2. Traitement des valeurs manquantes\n",
    "print(\"Traitement des valeurs manquantes...\")\n",
    "df_Xtrain.fillna(0, inplace=True)\n",
    "df_Xtest.fillna(0, inplace=True)\n",
    "df_Ytrain.fillna(0, inplace=True)\n",
    "df_Ytest.fillna(0, inplace=True)\n",
    "print(\"Valeurs manquantes traitÃ©es !\")\n",
    "\n",
    "# 3. Fusion des datasets\n",
    "print(\"Fusion des datasets...\")\n",
    "df_train = pd.concat([df_Xtrain, df_Ytrain], axis=1)\n",
    "df_test = pd.concat([df_Xtest, df_Ytest], axis=1)\n",
    "print(\"Fusion terminÃ©e !\")\n",
    "\n",
    "# 4. Chargement du modÃ¨le et du processeur BLIP pour l'extraction d'images\n",
    "print(\"Chargement du modÃ¨le BLIP...\")\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipModel.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model.eval()  # Mode Ã©valuation pour Ã©viter la mise Ã  jour des poids\n",
    "print(\"ModÃ¨le BLIP chargÃ© avec succÃ¨s !\")\n",
    "\n",
    "# 5. Fonction pour extraire les caractÃ©ristiques d'une image\n",
    "def extract_image_features(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = model.visual_encoder(**inputs)\n",
    "\n",
    "    return features.squeeze().numpy()\n",
    "\n",
    "# 6. Extraire les caractÃ©ristiques des images d'entraÃ®nement\n",
    "image_train_folder = \"Data/images/image_train\"\n",
    "image_train_features = []\n",
    "\n",
    "print(\"Extraction des caractÃ©ristiques des images d'entraÃ®nement...\")\n",
    "for image_file in os.listdir(image_train_folder):\n",
    "    image_path = os.path.join(image_train_folder, image_file)\n",
    "    image_features = extract_image_features(image_path)\n",
    "    image_train_features.append(image_features)\n",
    "print(f\"CaractÃ©ristiques des images d'entraÃ®nement extraites : {len(image_train_features)} images.\")\n",
    "\n",
    "# 7. Extraire les caractÃ©ristiques des images de test\n",
    "image_test_folder = \"Data/images/image_test\"\n",
    "image_test_features = []\n",
    "\n",
    "print(\"Extraction des caractÃ©ristiques des images de test...\")\n",
    "for image_file in os.listdir(image_test_folder):\n",
    "    image_path = os.path.join(image_test_folder, image_file)\n",
    "    image_features = extract_image_features(image_path)\n",
    "    image_test_features.append(image_features)\n",
    "print(f\"CaractÃ©ristiques des images de test extraites : {len(image_test_features)} images.\")\n",
    "\n",
    "# 8. Chargement du modÃ¨le BERT pour l'extraction de caractÃ©ristiques textuelles\n",
    "print(\"Chargement du modÃ¨le BERT...\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model.eval()\n",
    "print(\"ModÃ¨le BERT chargÃ© avec succÃ¨s !\")\n",
    "\n",
    "# 9. Fonction pour extraire les caractÃ©ristiques textuelles avec BERT\n",
    "def extract_text_features(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "\n",
    "# 10. Extraction des caractÃ©ristiques des textes\n",
    "print(\"Extraction des caractÃ©ristiques des textes...\")\n",
    "text_train_features = [extract_text_features(text) for text in df_train['text_column']]\n",
    "text_test_features = [extract_text_features(text) for text in df_test['text_column']]\n",
    "print(f\"CaractÃ©ristiques des textes extraites : {len(text_train_features)} textes.\")\n",
    "\n",
    "# 11. Fusion des caractÃ©ristiques images et textes\n",
    "print(\"Fusion des caractÃ©ristiques d'images et de texte...\")\n",
    "image_train_features = np.array(image_train_features)\n",
    "image_test_features = np.array(image_test_features)\n",
    "text_train_features = np.array(text_train_features)\n",
    "text_test_features = np.array(text_test_features)\n",
    "\n",
    "X_train_features = np.concatenate([image_train_features, text_train_features], axis=1)\n",
    "X_test_features = np.concatenate([image_test_features, text_test_features], axis=1)\n",
    "print(\"Fusion terminÃ©e.\")\n",
    "\n",
    "# 12. EntraÃ®nement du modÃ¨le RandomForest\n",
    "print(\"EntraÃ®nement du modÃ¨le RandomForest...\")\n",
    "model_rf = RandomForestClassifier(n_estimators=100)\n",
    "model_rf.fit(X_train_features, df_train['target_column'])\n",
    "\n",
    "# 13. Ã‰valuation du modÃ¨le\n",
    "accuracy = model_rf.score(X_test_features, df_test['target_column'])\n",
    "print(f\"PrÃ©cision du modÃ¨le : {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46b8da1-1a0b-47fe-9d60-d8bab11a6759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417fab0b-0d4b-4287-9458-595899c34bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74220030-0ea4-4dab-a57b-d8cdda33befd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des donnÃ©es...\n",
      "DonnÃ©es chargÃ©es avec succÃ¨s !\n",
      "Traitement des valeurs manquantes...\n",
      "Fusion des datasets...\n",
      "Vectorisation des descriptions textuelles...\n",
      "Chargement du modÃ¨le CNN pour l'extraction des features...\n",
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
      "\u001b[1m16705208/16705208\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step \n",
      "Extraction des features pour 84916 images...\n",
      "Extraction des features pour 13812 images...\n",
      "RÃ©duction de dimension des features d'images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a740407\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:685: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = self.explained_variance_ / total_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion des features texte et image...\n",
      "CrÃ©ation du pipeline et dÃ©but de l'entraÃ®nement...\n",
      "ModÃ¨le entraÃ®nÃ© en 757.33 secondes.\n",
      "PrÃ©diction et Ã©valuation du modÃ¨le...\n",
      "PrÃ©cision du modÃ¨le : 0.0036\n",
      "Fin du processus ! ðŸŽ¯\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# âœ… Chargement des donnÃ©es\n",
    "print(\"Chargement des donnÃ©es...\")\n",
    "df_Xtrain = pd.read_csv(\"Data/X_train_update.csv\")\n",
    "df_Xtest = pd.read_csv(\"Data/X_test_update.csv\")\n",
    "df_Ytrain = pd.read_csv(\"Data/Y_train.csv\")\n",
    "df_Ytest = pd.read_csv(\"Data/Y_test.csv\")\n",
    "print(\"DonnÃ©es chargÃ©es avec succÃ¨s !\")\n",
    "\n",
    "# âœ… Traitement des valeurs manquantes\n",
    "print(\"Traitement des valeurs manquantes...\")\n",
    "df_Xtrain['description'] = df_Xtrain['description'].fillna(\"aucune description\")\n",
    "df_Xtest['description'] = df_Xtest['description'].fillna(\"aucune description\")\n",
    "\n",
    "# âœ… Fusion des features avec les labels\n",
    "print(\"Fusion des datasets...\")\n",
    "df_train = df_Xtrain.merge(df_Ytrain, on=\"Unnamed: 0\").drop(columns=[\"Unnamed: 0\"])\n",
    "df_test = df_Xtest.merge(df_Ytest, on=\"Unnamed: 0\").drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "# âœ… Extraction des features textuelles\n",
    "print(\"Vectorisation des descriptions textuelles...\")\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train_text = vectorizer.fit_transform(df_train[\"description\"]).toarray()\n",
    "X_test_text = vectorizer.transform(df_test[\"description\"]).toarray()\n",
    "\n",
    "# âœ… Chargement du modÃ¨le de feature extraction d'images\n",
    "print(\"Chargement du modÃ¨le CNN pour l'extraction des features...\")\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, pooling='avg')\n",
    "model = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "# âœ… Fonction d'extraction des features d'une image\n",
    "def extract_image_features(image_path):\n",
    "    try:\n",
    "        img = image.load_img(image_path, target_size=(224, 224))  \n",
    "        img_array = image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)  \n",
    "        img_array = preprocess_input(img_array)  \n",
    "        features = model.predict(img_array)  \n",
    "        return features.flatten()\n",
    "    except:\n",
    "        return np.zeros((1280,))  # Retourne un vecteur nul si l'image est absente ou corrompue\n",
    "\n",
    "# âœ… Chargement des features des images\n",
    "def get_image_features(df, image_folder):\n",
    "    print(f\"Extraction des features pour {len(df)} images...\")\n",
    "    image_features = []\n",
    "    for img_id in df[\"imageid\"]:\n",
    "        img_path = os.path.join(image_folder, f\"{img_id}.jpg\")\n",
    "        image_features.append(extract_image_features(img_path))\n",
    "    return np.array(image_features)\n",
    "\n",
    "# âœ… Extraction des features d'images\n",
    "X_train_img = get_image_features(df_train, \"Data/images/image_train\")\n",
    "X_test_img = get_image_features(df_test, \"Data/images/image_test\")\n",
    "\n",
    "# âœ… RÃ©duction de dimension des features d'images (optionnel)\n",
    "print(\"RÃ©duction de dimension des features d'images...\")\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=256)  # On rÃ©duit Ã  256 dimensions\n",
    "X_train_img = pca.fit_transform(scaler.fit_transform(X_train_img))\n",
    "X_test_img = pca.transform(scaler.transform(X_test_img))\n",
    "\n",
    "# âœ… Fusion des features textuelles et visuelles\n",
    "print(\"Fusion des features texte et image...\")\n",
    "X_train = np.hstack((X_train_text, X_train_img))\n",
    "X_test = np.hstack((X_test_text, X_test_img))\n",
    "\n",
    "# âœ… DÃ©finition du modÃ¨le de classification\n",
    "print(\"CrÃ©ation du pipeline et dÃ©but de l'entraÃ®nement...\")\n",
    "classifier = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "\n",
    "# âœ… EntraÃ®nement du modÃ¨le\n",
    "start_time = time.time()\n",
    "classifier.fit(X_train, df_train[\"prdtypecode\"])\n",
    "training_time = time.time() - start_time\n",
    "print(f\"ModÃ¨le entraÃ®nÃ© en {training_time:.2f} secondes.\")\n",
    "\n",
    "# âœ… PrÃ©dictions et Ã©valuation\n",
    "print(\"PrÃ©diction et Ã©valuation du modÃ¨le...\")\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(df_test[\"prdtypecode\"], y_pred)\n",
    "print(f\"PrÃ©cision du modÃ¨le : {accuracy:.4f}\")\n",
    "\n",
    "print(\"Fin du processus ! ðŸŽ¯\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57664527-4c50-436a-9bd6-e715f967ebd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bfddee-ffbd-44ee-8c95-6b83067d3cac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac08ec11-537c-4561-817a-5f3d1a821d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4183cf08-8d2f-4264-a54c-3ac826b7caaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61af898-2f04-48a1-9f46-39231f985204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c66b3e-27ee-4b9d-890c-444a232c92c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aab6c16-266e-4b8d-8aef-70e6d2d8edc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46ed733-a379-4ca3-a321-6fdd95339d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4844e853-7af7-4219-afd0-ff6690621df9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7802a3-28b3-4450-bd4d-77181d4fb0c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afe463e-8631-4e21-82e1-a03c6818b666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c503f470-75b5-4ffe-9edc-22f755badb7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8b4ad8-fb15-4a3d-9f4f-55e74726e6bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b0a682-5884-4396-b9d3-6bd76aff9d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c069d85d-3a90-4aa4-8e0f-eb105a16b598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3546482c-39a3-42bf-ba13-5eedc902d969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f43ed1-000c-442d-98ce-2da1a6ae172a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa056759-1644-4779-8d69-db8b4a3116bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f0d85a-10f7-4fe3-8362-da284e5cbfab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc4e3e1-d04a-4f95-8434-a1f9185e08c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b944c0-3c94-4211-8b9c-5d7d7f84d0d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6db2a1-71fb-466d-83b5-a83b2e112220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
